{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir video_frames\n",
    "!mkdir  CLIP_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinyu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e4f584511c4dad83820ba9fad2d275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff961ead6504f28837167f09d008d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_chatglm.py:   0%|          | 0.00/17.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/glm-4v-9b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/home/xinyu/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687c6471f8a04fc899973315dd303ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/2.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda:7\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4v-9b\", trust_remote_code=True)\n",
    "\n",
    "query = \"\"\"You are a highly advanced assistant analyzing video frames and generating precise and contextually accurate descriptions of the actions occurring in each frame. Your task is to observe each video frame and produce a detailed caption that describes the current action taking place. Each description should be clear, specific, and concise, capturing the essence of the action accurately.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "Detail-Oriented Descriptions: Focus on the specific action being performed in the frame. Avoid general or vague descriptions. The static, non-interactive, background objects or scenes should NOT be your focus.\n",
    "Conciseness: Provide precise descriptions in one or two sentences.\n",
    "Consistency: Maintain a consistent style and level of detail throughout the video sequence.\"\"\"\n",
    "\n",
    "\n",
    "image = Image.open(\"cutting.jpg\").convert('RGB')\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"image\": image, \"content\": query}],\n",
    "                                       add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",\n",
    "                                       return_dict=True)  # chat mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad33fc2ed9d54281a703acc7272bca1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: A person's hands are shown expertly chopping green herbs with a chef's knife on a wooden cutting board. The chopping motion is swift and precise, with the knife's blade slicing through the herbs, leaving behind finely chopped pieces. <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = inputs.to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"THUDM/glm-4v-9b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "gen_kwargs = {\"max_length\": 256, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with open(\"captions.txt\", \"w\") as f:\n",
    "  for i, file in enumerate(tqdm(os.listdir(\"video_frames\"))):\n",
    "      inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"image\": Image.open(\"video_frames/\" + file).convert('RGB'), \"content\": query}],\n",
    "                                            add_generation_prompt=True, tokenize=True, return_tensors=\"pt\",\n",
    "                                              return_dict=True)  # chat mode\n",
    "      inputs = inputs.to(device)\n",
    "      with torch.no_grad():\n",
    "          outputs = model.generate(**inputs, **gen_kwargs)\n",
    "          outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "          caption = tokenizer.decode(outputs[0])\n",
    "          print(caption)\n",
    "          f.write(f\"{i}\\t\" + caption + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames in the video: 836\n",
      "Extracted 834 frames to video_frames\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def video_to_frames(video_path, output_folder):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video file opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total frames in the video: {frame_count}\")\n",
    "\n",
    "    frame_number = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save the frame as an image file\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{frame_number:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        frame_number += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_number} frames to {output_folder}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the video file\n",
    "    video_path = \"/home/xinyu/ActionSegmentation/cam01_P03_cereals.avi\"\n",
    "    \n",
    "    # Directory to save the extracted frames\n",
    "    output_folder = \"video_frames\"\n",
    "    \n",
    "    # Extract frames from the video\n",
    "    video_to_frames(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from math import trunc\n",
    "\n",
    "\n",
    "captions = []\n",
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.replace(\"Frame 1\", \"\")\n",
    "        line = line.replace(\"<|endoftext|>\", \"\")\n",
    "        captions.append(line.strip())   \n",
    "\n",
    "device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()\n",
    "text_inputs = torch.cat([clip.tokenize(c, truncate=True) for c in captions]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_features = text_features.cpu().numpy()\n",
    "\n",
    "name = 'P03_cam01_P03_cereals'\n",
    "pca = PCA(n_components=64)\n",
    "reduced_features = pca.fit_transform(text_features)\n",
    "print(\"reduced_features.shape: \", reduced_features.shape)  \n",
    "path = f'./CLIP_features/{name}.txt'\n",
    "np.savetxt(path, reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
